{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1 - Introduction to PyTorch \n",
    "### Course: DA5001/6400 (Jul-Nov 2024)\n",
    "### Course Instructor: Prof. Krishna Pillutla\n",
    "### Teaching Assistant: Ganesh S (ME20B070)\n",
    "PyTorch is a popular library for deep learning. This tutorial is designed for you to get familiar with PyTorch, by the end of which we will create a basic Neural Network, for classifying digits, trained on the MNIST dataset\n",
    "\n",
    "## Packages\n",
    "Please install the `torch` and `torchvision` packages [[Instructions](https://pytorch.org/get-started/locally/)].\n",
    "We recommend that you use [conda](https://docs.anaconda.com/miniconda/) as your package manager.\n",
    "\n",
    "## Exercises\n",
    "Please complete the `TODO` sections in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the MNIST data without using pytorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "# Import the dataset from torchvision.datasets\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True)\n",
    "\n",
    "X_train, y_train = train_dataset.data, train_dataset.targets.long()\n",
    "X_test, y_test = test_dataset.data, test_dataset.targets.long()\n",
    "\n",
    "# Flatten each image in X_train and X_test\n",
    "# Normalize from pixels [0, 255] -> [0, 1]\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).float() / 256\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).float() / 256\n",
    "\n",
    "# Print the shapes of the training and test datasets\n",
    "print(f'Train images shape: {X_train.shape}')\n",
    "print(f'Train labels shape: {y_train.shape}')\n",
    "print(f'Test images shape: {X_test.shape}')\n",
    "print(f'Test labels shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We take only 10% of the data, to avoid computational bottlenecks, and reduce training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify the code below to use 6000 random samples.\n",
    "# Note to sample input-output pairs from the same indices.\n",
    "X_train, y_train = X_train[:6000], y_train[:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 10\n",
    "plt.figure(figsize=(10, 1))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(1, num_images, i+1)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(y_train[i].item())\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the architecture of the Neural Network that will be used as a classification model for the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, transform=None):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.output = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size=28*28, hidden_size=100, output_size=10)\n",
    "\n",
    "# To see the model parameters for each layer of the network\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, shape: {param.shape}\")\n",
    "\n",
    "# To predict the output of a single image, before training\n",
    "model.eval()\n",
    "output = model(torch.tensor(X_train[0]).float())\n",
    "prediction = torch.argmax(output, dim=1).item() \n",
    "plt.imshow(X_train[0].reshape(28, 28), cmap='gray')\n",
    "print(f'Prediction: {prediction}')\n",
    "\n",
    "# Note that the prediction will likely be incorrect,\n",
    "# as the model has not been trained yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data, test_labels, batch_size):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    num_samples = len(test_labels)\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            batch_images = test_data[start_idx:start_idx + batch_size]\n",
    "            batch_labels = test_labels[start_idx:start_idx + batch_size]\n",
    "\n",
    "            outputs = model(batch_images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # TODO: calculate the number of correct predictions in this batch.\n",
    "            # Then increment `correct_predictions` by this amount.\n",
    "            correct_predictions += # TODO: your code here.\n",
    "\n",
    "    accuracy = correct_predictions / num_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using Stochastic Gradient descent\n",
    "\n",
    "The Stochastic Gradient Descent update rule is as follows:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(f_\\theta(X_B), y_B)\n",
    "$$\n",
    "\n",
    "Where \n",
    "- $\\theta$ is the set of parameters, \n",
    "- $f_\\theta$ is the function represented by the Neural Network, \n",
    "- $X_B$ and $y_B$ is a batch of input data and labels, sampled randomly from the training dataset and \n",
    "- $\\mathcal{L}$ is the Loss function\n",
    "\n",
    "The above update is done for each iteration until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Complete the following functions `train_model` and `evaluate_model` to train the model using Stochastic Gradient Descent and evaluate the model on the test set. Use three different learning rates:\n",
    "\n",
    "1. 5.0\n",
    "2. 0.5\n",
    "3. 0.005\n",
    "\n",
    "\n",
    "What do you observe? Note that we usually want a learning rate as large as possible such that the loss actually decreases. Which of these learning rates satisfy that criteria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, labels, batch_size, learning_rate, num_iterations):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    num_samples = len(labels)\n",
    "    indices = list(range(num_samples))\n",
    "\n",
    "    losses = []\n",
    "    test_accuracies = []\n",
    "    for itr in range(num_iterations):\n",
    "        # choose random set of integers of size batch_size\n",
    "        batch_indices = np.random.choice(indices, size=batch_size, replace=False)\n",
    "        batch_images = train_data[batch_indices]\n",
    "        batch_labels = labels[batch_indices]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_images)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        # Compute gradients using autograd\n",
    "        grads = torch.autograd.grad(outputs=loss, inputs=model.parameters(), create_graph=False)\n",
    "        # This is a list of tensors. Each tensor is the gradient with respect to \n",
    "        # corresponding parameter from `model.parameters()`.\n",
    "        \n",
    "        # Update parameters\n",
    "        with torch.no_grad():\n",
    "            for param, grad in zip(model.parameters(), grads):\n",
    "                # TODO: your code here\n",
    "                # Note: you need to update the parameters of the model using the gradients in `grads`.\n",
    "                # The update has to be in-place.\n",
    "                pass\n",
    "                \n",
    "\n",
    "        iteration_loss = loss.item()\n",
    "        test_accuracies.append(evaluate_model(model, X_test, y_test, batch_size))\n",
    "        losses.append(iteration_loss)\n",
    "        if(itr % 100 == 99):\n",
    "            print(f'Iteration [{itr + 1}/{num_iterations}], Loss: {iteration_loss:.4f}, Test accuracy: {test_accuracies[-1]:.4f}')\n",
    "\n",
    "    return losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = MLP(input_size=28*28, hidden_size=200, output_size=10)\n",
    "losses, test_accuracies = train_model(\n",
    "    model, X_train, y_train, batch_size=100, learning_rate=0.005, num_iterations=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, num_epochs+1 ), losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss in each iteration')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 1000+1), test_accuracies)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.title('Test Accuracy in each iteration')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Vary the hidden width of the neural network. At what width do we observe interpolation? Recall that interpolation is when the training loss is exactly zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
